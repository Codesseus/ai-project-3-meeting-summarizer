# -*- coding: utf-8 -*-
"""timestamp_extractor.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zF_9j1PDBstdTdzxl8OaitIvFKUPu9_k
"""

"""
Instructions:
1. Replace the `summary_text` variable with your processed summary text.
2. Replace the `content_text` variable with your processed content text.
3. Run the script to execute the entire process using the provided text.
4. Summary and content text must be fed in exactly as shown in the example with
  hyphens before bullet points and timestamps in the same syntax shown in the example.

example:
from timestamp_extractor import process_summary

summary_text =
- Bullet point 1
- Bullet point 2
- Bullet point 3

content_text =
[00:00:00] Line 1
[00:00:10] Line 2
[00:00:20] Line 3

process_summary(summary_text, content_text)
_______________________________________________________________________________

expected output:
The closest match(es) to '- Bullet point 1' in summary_text is:
[Timestamp A] is equivalent to [Seconds A] seconds.

The closest match(es) to '- Bullet point 2' in summary_text is:
[Timestamp B] is equivalent to [Seconds B] seconds.

The closest match(es) to '- Bullet point 3' in summary_text is:
[Timestamp C] is equivalent to [Seconds C] seconds.

"""

import re
import nltk
import numpy as np
from sentence_transformers import SentenceTransformer

# Download the necessary NLTK resources
nltk.download('punkt')

def process_summary(summary_text, content_text):
    # Preprocess the content text to extract lines and timestamps
    lines_with_timestamps = content_text.strip().split('\n')

    # Preprocess the summary text to extract bullet points
    bullet_points = []
    for line in summary_text.split('\n'):
        if line.startswith('- '):
            cleaned_line = re.sub(r'[-,\.]+', '', line.strip())
            bullet_points.append(cleaned_line)

    # Tokenize the lines from content text
    tokenized_lines = [nltk.word_tokenize(line) for line in lines_with_timestamps]

    # Convert tokenized lines to strings
    line_strs = [' '.join(line) for line in tokenized_lines]

    # Initialize the Sentence Transformer model
    model = SentenceTransformer('bert-base-nli-mean-tokens')

    # Compute embeddings for bullet points and lines
    bullet_point_embeddings = model.encode(bullet_points)
    line_embeddings = model.encode(line_strs)

    # Function to convert timestamp to seconds
    def timestamp_to_seconds(timestamp):
        hours, minutes, seconds = map(int, timestamp.strip('[]').split(':'))
        total_seconds = hours * 3600 + minutes * 60 + seconds
        return total_seconds

    # Iterate over all closest matches for each bullet point
    for j, bullet_point_embedding in enumerate(bullet_point_embeddings):
        closest_match_indices = []
        highest_similarity = -1

        # Find all closest matches
        for i, line_embedding in enumerate(line_embeddings):
            similarity = np.dot(bullet_point_embedding, line_embedding) / (np.linalg.norm(bullet_point_embedding) * np.linalg.norm(line_embedding))
            if similarity > highest_similarity:
                highest_similarity = similarity
                closest_match_indices = [i]
            elif similarity == highest_similarity:
                closest_match_indices.append(i)

        # Convert timestamps to seconds for all closest matches
        if closest_match_indices:
            print(f"The closest match(es) to '{bullet_points[j]}' in file1_contents is:")
            for idx in closest_match_indices:
                timestamp = lines_with_timestamps[idx].split(']')[0] + ']'
                seconds = timestamp_to_seconds(timestamp)
                print(f"{timestamp} is equivalent to {seconds} seconds.")
            print()
        else:
            print(f"No close match found for '{bullet_points[j]}' in file1_contents.")