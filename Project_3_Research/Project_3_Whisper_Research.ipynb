{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 3 research\n",
    "\n",
    "\n",
    "# Load environment variables.\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "# Video transcription with Whisper\n",
    "from openai import OpenAI\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources/Source_Video/Example_3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 27/6861 [01:00<4:14:18,  2.23s/it, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Resources/Output_Audio/Example_3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 27/6861 [01:05<4:37:53,  2.44s/it, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources/Output_Audio/Example_2.mp3\n",
      "Meet John, a talented programmer who was looking to start a company that used his personally developed mobile application to connect restaurants and customers for booking and reservations. Even though the app was ready, John had difficulty getting together a team for his startup, needing separate people for sales, marketing, programming, content creation, and customer support. Hiring reliable manpower while being strict with his budget was getting difficult. He reached out to his friend Ryan, who said John could start his company without hiring any new people, thanks to just a single AI-based tool. John couldn't believe it, which led Ryan to introduce ChatGPT, the revolutionary AI chatbot being developed by OpenAI. It is a state-of-the-art natural language processing, or NLP, model that uses a neural network architecture to provide responses. This means that the ChatGPT bot can answer questions without being explicitly told what the answer is using its own intellect, unlike previous AI chatbots. So, how does ChatGPT help John in filling out his team? Regarding sales, ChatGPT can provide full-fledged sales pitches based on the correct prompts. It can provide tips on how to pitch your product to businesses, removing the need for sales Completely customized to your requirements and your prompts, if you don't like some things about the response, you can ask for certain changes, and the chatbot will make sure they are done. When it comes to marketing, ChatGPT can provide efficient marketing strategies, which can help new entrepreneurs learn how to market their products to prospective clients. It can provide trending keywords that marketers can use for SEO purposes, while providing ad copies for your website and blog. Speaking of websites, since John can do a lot of the heavy lifting in programming, ChatGPT can help proofread the code and help out when looking for bugs to fix. Apart from basic bug fixing, it can also provide sample code structures for different programming languages, allowing John to focus more on improving core functionality and workflow rather than fixing basic code errors. Websites and blogs content is very helpful in gathering potential customer leads. The revolutionary bot can provide full-length blog posts with near-perfect fast accuracy in seconds, allowing further customization, like choosing the length of a subject matter to the complexity of language. For John's customer support, the bot can draft complete customer service emails based on the situation, saving time and resources. The tone of the message can be changed to reflect the nature of the message, creating an efficient alternative for call-centered professionals. John was left speechless seeing this level of versatility from ChatGPT and wanted to implement it right away. However, Ryan made sure John knew about some drawbacks of the chatbot before getting started. Since the bot is trained mostly on data up to 2021, many of the newer events may still need to be discovered by ChatGPT. Even basic stuff like asking about the current date and time is beyond its scope. Much like the limited understanding of context despite providing near-lifelike solutions to certain problems. Even the accuracy of many responses can be questioned since the AI model is still learning and being developed. There is a section of the public that believes the revolutionary tool can one day replace Google Search. But that day seems far-fetched so far because of the variety of issues people keep running into when using ChatGPT. So here's a question for you. Which of the following tasks cannot be carried out by ChatGPT? 1. Provide latest news 2. Complex mathematical calculations 3. Food recipes 4. None of the above Think about it and leave your answers in the comment section and we will provide the answer next week. Give the correct answer along with your reasoning and stand a chance to win an Amazon voucher. However, ChatGPT poses a lot of promise for the future of AI. From full-scaled automated divisions and organizations to serving as the perfect digital assistant, OpenAI is creating a bot for the future aimed at solving the problems of today with the tools of tomorrow. The ability to carry out a myriad of tasks with minimum manpower will boost productivity at organizations in every sector, thanks to the revolutionary ChatGPT. So how do you think ChatGPT will benefit your daily life? Are you looking forward to using the bot regularly for work or personal life? Let us know your thoughts in the comments below. We hope you enjoyed this video. If you did, a thumbs up would be really appreciated. Here's a reminder to subscribe to our channel and click on the bell icon for more on the latest technologies and trends. Thank you for watching and stay tuned for more from SimplyLearn.\n"
     ]
    }
   ],
   "source": [
    "audioSourceFolder = \"Resources/Output_Audio/\"\n",
    "sourceFilename = \"Example_2.mp3\"\n",
    "print(f\"{audioSourceFolder}{sourceFilename}\")\n",
    "audio_file = open(f\"{audioSourceFolder}{sourceFilename}\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file\n",
    ")\n",
    "\n",
    "\"\"\" Moved into functions below\n",
    "print(transcript.text)\n",
    "\n",
    "# save transcript to file\n",
    "outputFolder = \"Resources/Output_Text/\"\n",
    "outputFilename = sourceFilename.replace('.mp3', '.txt')\n",
    "outputFile = open(f\"{outputFolder}{outputFilename}\", \"w\")\n",
    "outputFile.write(transcript.text)\n",
    "outputFile.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Resources/Output_Audio/Example_2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n# Display file to be converted to text\\n# print(f\"{audioFolder}{audioFile}\")\\n\\n# Audio to Text (memory)\\naudio_file = open(f\"{audioFolder}{audioFile}\", \"rb\")\\ntranscript = client.audio.transcriptions.create(\\n    model=\"whisper-1\",\\n    file=audio_file\\n)\\n\\n\\n# Display transcript\\n# print(transcript.text)\\n\\n# Save Transcript Text to file\\ntextFolder = \"Resources/Output_Text/\"\\ntextFile = audioFile.replace(\\'.mp3\\', \\'.txt\\')\\noutputFile = open(f\"{textFolder}{textFile}\", \"w\")\\noutputFile.write(transcript.text)\\noutputFile.close()\\naudio.close()\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine video->Audio->Text\n",
    "\n",
    "# Video to Audio\n",
    "videoFolder = \"Resources/Source_Video/\"\n",
    "audioFolder = \"Resources/Output_Audio/\"\n",
    "videoFile = \"Example_2.mp4\"\n",
    "video = VideoFileClip(f\"{videoFolder}{videoFile}\")\n",
    "audio = video.audio\n",
    "\n",
    "\n",
    "audioFile = videoFile.replace('.mp4', '.mp3')\n",
    "audio.write_audiofile(f\"{audioFolder}{audioFile}\")\n",
    "\n",
    "video.close()\n",
    "\n",
    "\"\"\" \n",
    "# Display file to be converted to text\n",
    "# print(f\"{audioFolder}{audioFile}\")\n",
    "\n",
    "# Audio to Text (memory)\n",
    "audio_file = open(f\"{audioFolder}{audioFile}\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file\n",
    ")\n",
    "\n",
    "\n",
    "# Display transcript\n",
    "# print(transcript.text)\n",
    "\n",
    "# Save Transcript Text to file\n",
    "textFolder = \"Resources/Output_Text/\"\n",
    "textFile = audioFile.replace('.mp3', '.txt')\n",
    "outputFile = open(f\"{textFolder}{textFile}\", \"w\")\n",
    "outputFile.write(transcript.text)\n",
    "outputFile.close()\n",
    "audio.close()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to Transcribe audio with time stamps\n",
    "# Source: https://community.openai.com/t/how-to-get-whispers-api-to-add-timestamps-to-the-transcripts/501788\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Transcribe audio witout time stamps\n",
    "\n",
    "\n",
    "def transcribe_audio_no_time_stamps(client, audioFile):\n",
    "    audio_file = open(audioFile, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file\n",
    "    )\n",
    "    return transcript.text\n",
    "\n",
    "\n",
    "# transcribe audio with time stamps\n",
    "def transcribe_audio_with_time_stamps(client, file_path):\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"srt\"\n",
    "        )\n",
    "        # Pass the transcription directly for processing\n",
    "        return process_transcription(transcription)\n",
    "        # return response  # Directly return the response, assuming it's the transcription text\n",
    "\n",
    "\n",
    "def format_time(time):\n",
    "    time_obj = datetime.strptime(time, \"%H:%M:%S,%f\")\n",
    "    return time_obj.strftime(\"%H:%M:%S\")\n",
    "\n",
    "# Function to process the raw transcription into the desired format\n",
    "\n",
    "\n",
    "def process_transcription(transcription):\n",
    "    blocks = transcription.split('\\n\\n')\n",
    "    processed_lines = []\n",
    "    for block in blocks:\n",
    "        lines = block.split('\\n')\n",
    "        if len(lines) >= 3:\n",
    "            time_range = lines[1]\n",
    "            text = lines[2]\n",
    "            start_time = time_range.split(' --> ')[0]\n",
    "            # Convert the time format from \"00:00:00,000\" to \"0:00:00\"\n",
    "            formatted_start_time = format_time(start_time)\n",
    "            processed_line = f\"[{formatted_start_time}]{text}\"\n",
    "            processed_lines.append(processed_line)\n",
    "    return '\\n'.join(processed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Resources/Output_Audio/Example_2.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m audioFile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample_2.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Transcribe audio with time stamps\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m transcript_with_timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio_with_time_stamps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maudioFolder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maudioFile\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Transcribe audio without time stamps\u001b[39;00m\n\u001b[0;32m     11\u001b[0m transcript_no_timestamps \u001b[38;5;241m=\u001b[39m transcribe_audio_no_time_stamps(\n\u001b[0;32m     12\u001b[0m     client, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudioFolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00maudioFile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtranscribe_audio_with_time_stamps\u001b[1;34m(client, file_path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_audio_with_time_stamps\u001b[39m(client, file_path):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m audio_file:\n\u001b[0;32m     21\u001b[0m         transcription \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mtranscriptions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     22\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhisper-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m             file\u001b[38;5;241m=\u001b[39maudio_file,\n\u001b[0;32m     24\u001b[0m             response_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m         )\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m# Pass the transcription directly for processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dgerhart\\.conda\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Resources/Output_Audio/Example_2.mp3'"
     ]
    }
   ],
   "source": [
    "# Transcribe using methods above\n",
    "audioFolder = \"Resources/Output_Audio/\"\n",
    "textFolder = \"Resources/Output_Text/\"\n",
    "audioFile = \"Example_2.mp3\"\n",
    "\n",
    "# Transcribe audio with time stamps\n",
    "transcript_with_timestamps = transcribe_audio_with_time_stamps(\n",
    "    client, f\"{audioFolder}{audioFile}\")\n",
    "\n",
    "# Transcribe audio without time stamps\n",
    "transcript_no_timestamps = transcribe_audio_no_time_stamps(\n",
    "    client, f\"{audioFolder}{audioFile}\")\n",
    "\n",
    "# Save transcript with time stamps to file\n",
    "textFile = audioFile.replace('.mp3', '_with_timestamps.txt')\n",
    "outputFile = open(f\"{textFolder}{textFile}\", \"w\")\n",
    "outputFile.write(transcript_with_timestamps)\n",
    "outputFile.close()\n",
    "\n",
    "# Save transcript without time stamps to file\n",
    "textFile = audioFile.replace('.mp3', '_no_timestamps.txt')\n",
    "outputFile = open(f\"{textFolder}{textFile}\", \"w\")\n",
    "outputFile.write(transcript_no_timestamps)\n",
    "outputFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
