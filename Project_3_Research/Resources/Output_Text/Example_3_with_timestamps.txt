[00:00:00]In the coming years, artificial intelligence is probably going to change your life and
[00:00:11]likely the entire world, but people have a hard time agreeing on exactly how.
[00:00:15]The following are excerpts from a World Economic Forum interview where renowned computer science
[00:00:20]professor and AI expert Stuart Russell helps separate the sense from the nonsense.
[00:00:25]There's a big difference between asking a human to do something and giving that as the
[00:00:30]objective to an AI system.
[00:00:32]When you ask a human to fetch you a cup of coffee, you don't mean this should be their
[00:00:37]life's mission and nothing else in the universe matters, even if they have to kill everybody
[00:00:41]else in Starbucks to get you the coffee before it closes.
[00:00:44]They should do that.
[00:00:45]No, that's not what you mean.
[00:00:46]You mean all the other things that we mutually care about, they should factor into your behavior
[00:00:50]as well.
[00:00:51]The problem with the way we build AI systems now is we give them a fixed objective, right?
[00:00:55]The algorithms require us to specify everything in the objective.
[00:00:59]And if you say, you know, can we fix the acidification of the oceans, yeah, you could have a catalytic
[00:01:04]reaction that does that extremely efficiently, but, you know, consumes a quarter of the oxygen
[00:01:09]in the atmosphere, which would apparently cause us to die fairly slowly and unpleasantly
[00:01:14]over the course of several hours.
[00:01:16]So how do we avoid this problem, right?
[00:01:19]You might say, okay, well, just be more careful about specifying the objective, right?
[00:01:23]Don't forget the atmospheric oxygen.
[00:01:26]And then of course, some side effect of the reaction in the ocean poisons all the fish.
[00:01:30]Okay, well, I meant don't kill the fish either.
[00:01:33]And then, well, what about the seaweed?
[00:01:35]Okay, don't do anything that's going to cause all the seaweed to die and on and on and on,
[00:01:39]right?
[00:01:40]And the reason that we don't have to do that with humans is that humans often know that
[00:01:45]they don't know all the things that we care about.
[00:01:48]If you ask a human to get you a cup of coffee, you know, and you happen to be in the hotel
[00:01:52]Georges Cinq in Paris, where the coffee is, I think, 13 euros a cup.
[00:01:57]It's entirely reasonable to come back and say, well, it's 13 euros, are you sure you
[00:02:01]want, or I could go next door and, you know, get one.
[00:02:03]And it's a perfectly normal thing for a person to do, right, to ask, you know, I'm going
[00:02:09]to repaint your house, is it okay if I take off the drain pipes and then put them back?
[00:02:13]We don't think of this as a terribly sophisticated capability, but AI systems don't have it,
[00:02:18]because the way we build them now, they have to know the full objective.
[00:02:22]If we build systems that know that they don't know what the objective is, then they start
[00:02:26]to exhibit these behaviors, like asking permission before getting rid of all the opps in the
[00:02:32]atmosphere.
[00:02:33]In all these senses, control over the AI system comes from the machine's uncertainty about
[00:02:39]what the true objective is.
[00:02:41]It's when you build machines that believe with certainty that they have the objective,
[00:02:46]it's when you get the sort of psychopathic behavior, and I think we see the same thing
[00:02:49]in humans.
[00:02:51]What happens when general purpose AI hits the real economy?
[00:02:55]How do things change?
[00:02:57]Can we adapt?
[00:02:59]This is a very old point.
[00:03:01]Amazingly, Aristotle actually has a passage where he says, look, if we had fully automated
[00:03:06]weaving machines and plectrums that could pluck the lyre and produce music without any
[00:03:10]humans, then we wouldn't need any workers.
[00:03:14]That idea, which I think it was Keynes who called it technological unemployment in 1930,
[00:03:19]is very obvious to people, right?
[00:03:21]They think, yeah, of course, if the machine does the work, then I'm going to be unemployed.
[00:03:26]If you think about the warehouses that companies are currently operating for e-commerce, they
[00:03:31]are half automated.
[00:03:32]The way it works is that an old warehouse where you've got tons of stuff piled up all
[00:03:36]over the place and humans go and rummage around and then bring it back and send it off, there's
[00:03:41]a robot who goes and gets the shelving unit that contains the thing that you need.
[00:03:46]But the human has to pick the object out of the bin or off the shelf because that's still
[00:03:51]too difficult.
[00:03:52]But, you know, at the same time, if you make a robot that is accurate enough to be able
[00:03:57]to pick pretty much any object with a very wide variety of objects that you can buy,
[00:04:02]that would, at a stroke, eliminate three or four million jobs.
[00:04:06]There's an interesting story that E.M. Forster wrote where everyone is entirely machine dependent.
[00:04:13]The story is really about the fact that if you hand over the management of your civilization
[00:04:19]to machines, you then lose the incentive to understand it yourself or to teach the next
[00:04:24]generation how to understand it.
[00:04:26]And you can see Wall-E actually as a modern version where everyone is enfeebled and infantilized
[00:04:31]by the machine.
[00:04:32]And that hasn't been possible up to now, right?
[00:04:35]We put a lot of our civilization into books, but the books can't run it for us.
[00:04:39]And so we always have to teach the next generation.
[00:04:41]If you work it out, it's about a trillion person years of teaching and learning and
[00:04:46]an unbroken chain that goes back tens of thousands of generations.
[00:04:50]What happens if that chain breaks?
[00:04:51]And I think that's something we have to understand as AI moves forward.
[00:04:55]The actual date of arrival of general purpose AI, you're not going to be able to pinpoint,
[00:05:00]right?
[00:05:01]It isn't a single day.
[00:05:02]It's also not the case that it's all or nothing.
[00:05:04]The impact is going to be increasing.
[00:05:07]So with every advance in AI, it significantly expands the range of tasks.
[00:05:12]So in that sense, I think most experts say by the end of the century, we're very, very
[00:05:18]likely to have general purpose AI.
[00:05:21]The median is something around 2045.
[00:05:24]I'm a little more on the conservative side.
[00:05:26]I think the problem is harder than we think.
[00:05:28]I like what John McCarthy, who was sort of one of the founders of AI, when he was asked
[00:05:32]this question, he said, well, somewhere between five and 500 years.
[00:05:36]And we're going to need, I think, several Einsteins to make it happen.
[00:05:40]But how will the economy change in the meantime?
[00:05:42]Will it be able to keep growing?
[00:05:44]Watch this video to hear economist Kate Raworth explain why growth is not always a good thing.
[00:05:50]Or continue to expand your understanding of economics on the World Economic Forum's YouTube
[00:05:54]channel.